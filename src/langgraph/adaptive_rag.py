
from typing import Literal, List

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain.schema import BaseOutputParser


class RouteQuery(BaseModel):
    """Route a user query to the most relevant datasource."""

    datasource: Literal["vectorstore", "no-retrieval"] = Field(
        ...,
        description="Given a user question choose to route it to a vectorstore or to the LLMs built in context.",
    )

### Query Analyzer

class QueryAnalyzer:
    """
    Classifies the user query to determine if it requires retrieval from the knowledge base
    or can be answered directly by the LLM.
    """
    def __init__(
        self,
        knowledge_base_description: str,
        model_name: str = "gpt-4o",
        temperature: float = 0
    ):
        # Initialize the LLM with function calling capabilities
        self.llm = ChatOpenAI(model=model_name, temperature=temperature, max_tokens = 4000)
        self.structured_llm_router = self.llm.with_structured_output(RouteQuery)
        
        # Define the system prompt with the user-provided knowledge base description
        self.system_prompt = f"""You are an expert at routing a user question to a vectorstore or to an LLM.
                                The vectorstore contains documents related to: {knowledge_base_description}
                                Use the vectorstore for questions on these topics. Otherwise, use no-retrieval."""
        
        # Create the prompt template
        self.route_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", self.system_prompt),
                ("human", "{question}"),
            ]
        )

        self.question_router = self.route_prompt | self.structured_llm_router

    def analyze_query(self, question: str) -> str:
        """
        Determines the query's complexity and relevance.
        Returns 'vectorstore' or 'no-retrieval' based on the analysis.
        """
        # Format the prompt with the user's question
        formatted_prompt = self.route_prompt.format(question=question)
        
        # Get the structured output from the LLM
        response = self.structured_llm_router(formatted_prompt)
        
        # Parse the response into the RouteQuery model
        route_query = RouteQuery.parse_raw(response.content)
        
        return route_query.datasource
    



### Answer Grader

class GradeAnswer(BaseModel):
    """Binary score to assess answer addresses question."""

    binary_score: str = Field(
        description="Answer addresses the question, 'yes' or 'no'"
    )

class AnswerGrader:
    """
    Grades an LLM-generated answer to determine if it adequately addresses the user's question.
    """
    def __init__(
        self,
        knowledge_base_description: str,
        model_name: str = "gpt-4o",
        temperature: float = 0
    ):
        
        
        # Initialize the LLM with function calling capabilities
        self.llm = ChatOpenAI(model=model_name, temperature=temperature, max_tokens = 4000)
        self.structured_llm_grader = self.llm.with_structured_output(RouteQuery)

        self.system_prompt = """You are a grader assessing whether an answer addresses / resolves a question \n 
                    Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question."""
        self.answer_prompt = ChatPromptTemplate.from_messages(
            [
                    ("system", self.system_prompt),
                    ("human", "User question: \n\n {question} \n\n LLM generation: {generation}"),
            ]
        )

        self.answer_grader = self.answer_prompt | self.structured_llm_grader

    def grade_answer(self, question: str, generation: str) -> str:
        """
        Grades the answer generated by the LLM to determine if it resolves the user's question.
        Returns 'yes' or 'no' based on the grading.
        """
        # Format the prompt with the question and the generated answer
        formatted_prompt = self.answer_prompt.format(question=question, generation=generation)
        
        # Get the structured output from the LLM
        response = self.structured_llm_grader(formatted_prompt)
        
        # Parse the response into the GradingResult model
        grading_result = GradeAnswer.parse_raw(response.content)
        
        return grading_result.score.lower()


### Rewrite Question

class StrOutputParser(BaseOutputParser):
    """Simple output parser that returns the text as is."""
    def parse(self, text: str) -> str:
        return text.strip()
    
# # class SubQueries(BaseModel):
#     sub_queries: List[str] = Field(description="List of sub-queries generated to help answer the user query.")

class RewrittenQuery(BaseModel):
    rewritten_query: str = Field(
        ...,
        description="The rewritten query optimized for vectorstore retrieval."
    )

class StepBackQuery(BaseModel):
    stepback_query: str = Field(
        ...,
        description="A more general query to help retrieve relevant background information."
    )

class SubQueries(BaseModel):
    subqueries: List[str] = Field(
        ...,
        description="A list of 2-4 simpler sub-queries."
    )

class QueryTransformer:
    """
    Rewrites the input question to a better version optimized for vectorstore retrieval.
    Generates a step-back query to get broad context
    Decompose original query into simpler subqueries
    """
    def __init__(self,
                model_name: str = "gpt-4o"
                ):
        # Initialize the LLM
        self.rewrite_llm = ChatOpenAI(model=model_name, temperature=0)
        self.stepback_llm = ChatOpenAI(model=model_name, temperature=0)
        self.subquery_llm = ChatOpenAI(model=model_name, temperature=0.3)


        # Define the system prompts
        self.rewrite_system_prompt = (
            """You are an AI assistant that converts an input question to a better version 
            that is optimized for vectorstore retrieval. Look at the input and try to reason 
            about the underlying semantic intent/meaning."""
        )
        self.stepback_system_prompt = (
            """You are an AI assistant that takes input questions and generates more broad, 
            general queries to improve context retreival in the RAG system.
            Given the original query, generate a step-back query that is more general and can help retrieve relevant background information."""
        )
        self.subquery_system_prompt = (
            """You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.
            Given the original query, decompose it into 2-4 distinct, simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.

            Example: What TV shows should I watch?


            1. What are the top-rated shows across various streaming platforms?
            2. What shows are trending or highly recommended in popular genres (e.g., drama, comedy, thriller)?
            3. Are there any recently released shows that have received critical acclaim?
            4. What shows have won awards or received nominations in the past few years? 
            """
        )

        # Structured outputs for each query transform
        self.structured_llm_rewrite = self.llm.with_structured_output(RewrittenQuery)
        self.structured_llm_stepback = self.llm.with_structured_output(StepBackQuery)
        self.structured_llm_subqueries = self.llm.with_structured_output(SubQueries)


        # Create the prompt re-writer template
        self.rewrite_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", self.system_prompt),
                (
                    "human",
                    "Here is the initial question:\n\n{question}\n\nFormulate an improved question.",
                ),
            ]
        )
        
        # Create the prompt step-back template
        self.stepback_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", self.system_prompt),
                (
                    "human",
                    "Here is the initial question:\n\n{question}\n\nFormulate a more general question that captures the semantic meaning of the original.",
                ),
            ]
        )

        # Create the query decomp prompt template
        self.decomp_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", self.system_prompt),
                (
                    "human",
                    "Here is the initial question:\n\n{question}\n\nFormulate sub-questions that help answer the original question.",
                ),
            ]
        )


        # Create the pipeline: prompt -> llm -> output parser
        # self.rewrite_pipeline = self.rewrite_prompt | self.llm | StrOutputParser()
        # self.stepback_pipeline = self.stepback_prompt | self.llm | StrOutputParser()
        # self.decomp_pipeline = self.decomp_prompt | self.llm | StrOutputParser()

    def rewrite_question(self, question: str) -> str:
        """
        Rewrites the input question to improve vectorstore retrieval performance.
        """
        # Invoke the pipeline with the question
        rewritten_question = self.pipeline.invoke({"question": question})
        return rewritten_question
    

